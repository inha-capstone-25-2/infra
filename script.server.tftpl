#cloud-config
package_update: true

bootcmd:
  - mkdir -p /home/ubuntu/.kaggle
  - chown ubuntu:ubuntu /home/ubuntu/.kaggle
  - chmod 0700 /home/ubuntu/.kaggle

write_files:
  - path: /home/ubuntu/app/docker-compose.postgres.yml
    permissions: '0644'
    content: |
      version: '3.8'
      services:
        postgres:
          image: postgres:16
          container_name: postgres_prod
          restart: unless-stopped
          environment:
            - POSTGRES_USER=${postgres_username}
            - POSTGRES_PASSWORD=${postgres_password}
            - POSTGRES_DB=rsrs
            - PGDATA=/var/lib/postgresql/data
          ports:
            - "5432:5432"
          volumes:
            - postgres_data:/var/lib/postgresql/data
          networks:
            - inha-network
      volumes:
        postgres_data:
      networks:
        inha-network:
          driver: bridge
  - path: /home/ubuntu/backend/scripts/download-dataset.sh
    permissions: '0755'
    content: |
      #!/usr/bin/env bash
      set -euo pipefail

      # Use ~/.kaggle/kaggle.json
      export KAGGLE_CONFIG_DIR="$${KAGGLE_CONFIG_DIR:-/home/ubuntu/.kaggle}"

      # Terraform 렌더 기본값 주입(런타임 ENV가 우선)
      S3_BUCKET_DEFAULT="${s3_bucket_name}"
      S3_PREFIX_DEFAULT="arxiv"

      : "$${DATASET:=Cornell-University/arxiv}"
      : "$${FILENAME:=arxiv-metadata-oai-snapshot.json}"
      : "$${S3_BUCKET:=${S3_BUCKET_DEFAULT}}"
      : "$${S3_PREFIX:=${S3_PREFIX_DEFAULT}}"
      SNAPSHOT_DATE="$${SNAPSHOT_DATE:-$(date -u +%Y%m%d)}"

      snap_key="$S3_PREFIX/snapshots/$SNAPSHOT_DATE/arxiv-metadata-oai-snapshot.json.gz"
      latest_key="$S3_PREFIX/latest.json.gz"

      workdir="$${TMPDIR:-/tmp}/arxiv_sync.$$"
      mkdir -p "$workdir"
      echo "[*] workdir: $workdir"
      trap 'rm -rf "$workdir"' EXIT

      echo "[*] Downloading dataset from Kaggle..."
      kaggle datasets download -d "$DATASET" -f "$FILENAME" -p "$workdir"

      zip_path="$workdir/$FILENAME.zip"
      raw_path="$workdir/$FILENAME"

      echo "[*] Checking downloaded artifacts..."
      ls -lah "$workdir" || true

      if [ -s "$zip_path" ]; then
        echo "[*] Detected ZIP file: $zip_path"
        echo "[*] Streaming unzip -> pv -> gzip -> S3 upload..."
        unzip -p "$zip_path" "$FILENAME" \
          | pv -rab -i 2 \
          | gzip -6 \
          | aws s3 cp - "s3://$S3_BUCKET/$snap_key" \
               --content-type "application/json" \
               --content-encoding "gzip" \
               --only-show-errors
      elif [ -s "$raw_path" ]; then
        echo "[*] Detected raw file (no zip): $raw_path"
        echo "[*] Streaming raw -> pv -> gzip -> S3 upload..."
        pv -rab -i 2 "$raw_path" \
          | gzip -6 \
          | aws s3 cp - "s3://$S3_BUCKET/$snap_key" \
               --content-type "application/json" \
               --content-encoding "gzip" \
               --only-show-errors
      else
        echo "[!] Downloaded file not found (zip or raw)."
        exit 1
      fi

      echo "[*] Updating latest pointer s3://$S3_BUCKET/$latest_key"
      aws s3 cp "s3://$S3_BUCKET/$snap_key" "s3://$S3_BUCKET/$latest_key" --only-show-errors

      echo "[*] Done."
  - path: /home/ubuntu/.kaggle/kaggle.json
    permissions: '0600'
    owner: ubuntu:ubuntu
    content: |
      {"username":"${kaggle_api_username}","key":"${kaggle_api_key}"}
  - path: ${dataset_log_path}
    permissions: '0664'
    owner: ubuntu:ubuntu
    content: ""
  - path: /etc/cron.d/arxiv_sync
    permissions: '0644'
    content: |
      # Cron: 매일 KST 03:00 실행 (시스템 TZ=Asia/Seoul)
      SHELL=/bin/bash
      PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin
      HOME=/home/ubuntu
      KAGGLE_CONFIG_DIR=/home/ubuntu/.kaggle
      # Terraform이 렌더한 버킷 이름을 ENV로 주입
      S3_BUCKET=${s3_bucket_name}
      S3_PREFIX=arxiv
      DATASET=Cornell-University/arxiv
      FILENAME=arxiv-metadata-oai-snapshot.json
      LOG_FILE=${dataset_log_path}
      0 3 * * * ubuntu bash /home/ubuntu/backend/scripts/download-dataset.sh >> $LOG_FILE 2>&1

runcmd:
  - chown ubuntu:ubuntu /home/ubuntu
  - chmod 0755 /home/ubuntu
  - mkdir -p /home/ubuntu/backend
  - chown -R ubuntu:ubuntu /home/ubuntu/backend
  - mkdir -p /home/ubuntu/app
  - chown -R ubuntu:ubuntu /home/ubuntu/app

  - timedatectl set-timezone Asia/Seoul

  - bash -lc 'test -f /swapfile || (fallocate -l 8G /swapfile || dd if=/dev/zero of=/swapfile bs=1M count=8192)'
  - chmod 600 /swapfile
  - mkswap /swapfile || true
  - swapon /swapfile || true
  - bash -lc "grep -q '^/swapfile ' /etc/fstab || echo '/swapfile none swap sw 0 0' >> /etc/fstab"

  - apt-get update -y
  - curl -fsSL https://get.docker.com | sh
  - systemctl enable docker
  - systemctl start docker
  - apt-get update -y
  - apt-get install -y docker-compose-plugin unzip python3 python3-pip awscli pv
  - pip3 install --no-cache-dir kaggle

  - bash -lc 'until docker info >/dev/null 2>&1; do sleep 2; done'
  - docker compose -f /home/ubuntu/app/docker-compose.postgres.yml up -d

  # 로그 파일이 없으면 생성해 ubuntu가 쓸 수 있도록 보장
  - bash -lc 'test -f ${dataset_log_path} || install -m 0664 -o ubuntu -g ubuntu /dev/null ${dataset_log_path}'
  - chown ubuntu:ubuntu ${dataset_log_path} || true
  - chmod 0664 ${dataset_log_path} || true

  # 최초 1회 실행 (bash 명시 + S3_BUCKET/S3_PREFIX 주입)
  - su - ubuntu -c "env PATH=/usr/local/bin:/usr/bin:/bin KAGGLE_CONFIG_DIR=/home/ubuntu/.kaggle S3_BUCKET='${s3_bucket_name}' S3_PREFIX='arxiv' bash /home/ubuntu/backend/scripts/download-dataset.sh >> ${dataset_log_path} 2>&1 || true"
